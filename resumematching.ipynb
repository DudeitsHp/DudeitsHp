{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests \n",
    "import PyPDF2\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                data\n",
      "0  named entity real world object assigned name e...\n",
      "1  sql stand structured query language programmin...\n",
      "2  although blockchain still infancy opportunity ...\n",
      "3  job interview scary fresher especially attendi...\n",
      "4  memcached highly performant distributed cachin...\n"
     ]
    }
   ],
   "source": [
    "articles = pd.read_csv('data/data.csv')\n",
    "print(articles.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(articles['data'])\n",
    "tagged_data = [TaggedDocument(words = word_tokenize(_d.lower()), tags = [str(i)]) for i, _d in enumerate(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(vector_size = 50,\n",
    "min_count = 10,\n",
    "epochs = 50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab(tagged_data)\n",
    "k = list(model.wv.index_to_key)\n",
    "print(len(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "model.train(tagged_data,\n",
    "total_examples = model.corpus_count,\n",
    "epochs = model.epochs)\n",
    "model.save('data/doc2vec.model')\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_path = 'data/resume1.pdf'\n",
    "resume = ''\n",
    "pdfReader = PyPDF2.PdfReader(resume_path)\n",
    "for i in range(len(pdfReader.pages)):\n",
    "    pageObj = pdfReader.pages[i]\n",
    "    resume += pageObj.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = resume.lower()\n",
    "resume = re.sub('[^a-z]', ' ', resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vedansh shrivastavavedansh   gmail com                bhopal  dehradun education bachelor of technology  b tech   computer science with specialization in bigdata university of petroleum and energy studies             senior secondary  xii   science delhi public school  cbse board  year of completion       secondary  x  delhi public school  cbse board  year of completion       internships head of volunteers arushi ngo  bhopal may        jun      the internship was for handling the physically challenged students recording books for visually challenged students  trainings deep learning specialization deeplearning ai  online oct        nov      machine learning stanford university  coursera   online mar        may      apache spark with scala  hands on bigdata udemy  online feb        feb      projects face mask detection nov        nov      https   github com vedansh   face mask detection i used computer vison for this purpose by using pretrained network mobilenetv  and using mtcnn to make bounding box around the face  opencv was used for real time face mask detection face recognition nov        nov      https   github com vedansh   face recognitioni have used opencv for realtime video capture  mtcnn to detect the face and landmarks in the frame and at last used mobilenet v  pre trained model for training the dataset  images   email spam detection using ml and data science and python may        may      used ml and data preprocessing for cleaning up the data and used nlp natural language processing algorithms   and deep learning used gaussian nb algorithm for modeling the data  accuracy after the modeling on test data was        skills python intermediatemachine learning intermediate data analytics intermediatems o ce intermediate ms excel intermediatec   programming intermediate sql intermediatenatural language processing  nlp  intermediate big data analytics intermediateenglish pro ciency  spoken  advanced data science intermediateopencv intermediate neural networks intermediatedeep learning intermediate computer vision intermediateenglish pro ciency  written  advanced work samples github pro le https   github com vedansh   other portfolio link https   www linkedin com in vedansh shrivastava            additional details i have also worked on many datasets like titanic  census income dataset  black friday sales  loanprediction  ant vs bees  aerial cactus identication  mnist and many more which can be found on mygithub prole'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(url):\n",
    "    list1 = []\n",
    "    count = 0\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code == 200:\n",
    "        soup = BeautifulSoup(resp.text,'html.parser')\n",
    "        l = soup.find(class_ = 'av-company-description-page mb-2')\n",
    "        web = ''.join([i.text for i in l.find_all(['p', 'li'])])\n",
    "        list1.append(web)\n",
    "        return web\n",
    "    else:\n",
    "        print(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "jd_links = ['https://datahack.analyticsvidhya.com/jobathon/clix-capital/senior-manager-growthrisk-analytics-2',\n",
    "'https://datahack.analyticsvidhya.com/jobathon/clix-capital/manager-growth-analytics-2',\n",
    "'https://datahack.analyticsvidhya.com/jobathon/clix-capital/manager-risk-analytics-2',\n",
    "'https://datahack.analyticsvidhya.com/jobathon/cropin/data-scientist-85']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "jd_df = pd.DataFrame(columns = ['links', 'data'])\n",
    "jd_df['links'] = jd_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the JD data from the links:\n",
    "for i in range(len(jd_df)):\n",
    "    jd_df['data'][i] = extract_data(jd_df['links'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the text into lower case\n",
    "jd_df.loc[:,\"data\"] = jd_df.data.apply(lambda x : str.lower(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the punctuations from the text\n",
    "jd_df.loc[:,\"data\"] = jd_df.data.apply(lambda x : \" \".join(re.findall('[\\w]+',x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the numerics present in the text\n",
    "jd_df.loc[:,\"data\"] = jd_df.data.apply(lambda x : re.sub(r'\\d+','',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    clix capital company overviewclix capital is a...\n",
      "1    clix capital company overviewclix capital is a...\n",
      "2    clix capital company overviewclix capital is a...\n",
      "3    about uscropin at bangalore is an earth observ...\n",
      "Name: data, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(jd_df.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.505\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec.load('data/doc2vec.model')\n",
    "v1 = model.infer_vector(resume.split())\n",
    "v2 = model.infer_vector(jd_df['data'][0].split())\n",
    "cosine_similarity = (np.dot(np.array(v1), np.array(v2))) / ((np.linalg.norm(v1)) * (np.linalg.norm(v2)))\n",
    "print(round(cosine_similarity, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
